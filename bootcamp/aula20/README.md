# CRUD FASTAPI POSTGRES STREAMLIT

Voc√™ sabe o que √© CRUD?

![Imagem CRUD](assets/crud.jpeg)

A BlackFriday ta chegando. Voc√™ sabe como que o Iphone fica mais barato? Voc√™ sabe como que o v√≠deo game √© cadastrado? Voc√™ sabia que quando abre o seu navegador, nada mais √© do que o seu browser fazendo um SELECT no banco do Mercado Livre ü§Ø

Voc√™ precisa conhecer o CRUD.

O principal respons√°vel por tornar isso poss√≠vel √© o ORM

![Imagem ORM](assets/orm.png)

## Instala√ß√£o via docker

```bash
docker-compose up -d --build
```

### Uso

Frontend:
Acesse o endere√ßo http://localhost:8501

### Documenta√ß√£o

Backend:
Acesse o endere√ßo http://localhost:8000/docs

## Nossa estrutura de pastas e arquivos

```bash
‚îú‚îÄ‚îÄ README.md # arquivo com a documenta√ß√£o do projeto
‚îú‚îÄ‚îÄ backend # pasta do backend (FastAPI, SQLAlchemy, Uvicorn, Pydantic)
‚îú‚îÄ‚îÄ frontend # pasta do frontend (Streamlit, Requests, Pandas)
‚îú‚îÄ‚îÄ docker-compose.yml # arquivo de configura√ß√£o do docker-compose (backend, frontend, postgres)
‚îú‚îÄ‚îÄ poetry.lock # arquivo de lock do poetry
‚îî‚îÄ‚îÄ pyproject.toml # arquivo de configura√ß√£o do poetry
```

## Nosso Backend

Nosso backend vai ser uma API, que ser√° respons√°vel por fazer a comunica√ß√£o entre o nosso frontend com o banco de dados. Vamos detalhar cada uma das pastas e arquivos do nosso backend.

### FastAPI

O FastAPI √© um framework web para construir APIs com Python. Ele √© baseado no Starlette, que √© um framework ass√≠ncrono para construir APIs. O FastAPI √© um framework que est√° crescendo muito, e que tem uma curva de aprendizado muito baixa, pois ele √© muito parecido com o Flask.

### Uvicorn

O Uvicorn √© um servidor web ass√≠ncrono, que √© baseado no ASGI, que √© uma especifica√ß√£o para servidores web ass√≠ncronos. O Uvicorn √© o servidor web recomendado pelo FastAPI, e √© o servidor que vamos utilizar nesse projeto.

### SQLAlchemy

O SQLAlchemy √© uma biblioteca para fazer a comunica√ß√£o com o banco de dados. Ele √© um ORM (Object Relational Mapper), que √© uma t√©cnica de mapeamento objeto-relacional que permite fazer a comunica√ß√£o com o banco de dados utilizando objetos.

Uma das principais vantagens de trabalhar com o SQLAlchemy √© que ele √© compat√≠vel com v√°rios bancos de dados, como MySQL, PostgreSQL, SQLite, Oracle, Microsoft SQL Server, Firebird, Sybase e at√© mesmo o Microsoft Access.

Al√©m disso, ele realiza a sanitiza√ß√£o dos dados, evitando ataques de SQL Injection.

![imagem](assets/sqlinjection.jpeg)

Outro ponto, √© que voc√™ pode trabalhar com m√©todos nativos do Python, como por exemplo o filter, que √© muito utilizado para fazer filtros em listas. Isso facilita muito a nossa vida, pois n√£o precisamos aprender uma nova linguagem para fazer a comunica√ß√£o com o banco de dados. Quem tiver familidade com Pandas, vai se sentir em casa.

### Pydantic

O Pydantic √© uma biblioteca para fazer a valida√ß√£o de dados. Ele √© utilizado pelo FastAPI para fazer a valida√ß√£o dos dados que s√£o recebidos na API, e tamb√©m para definir os tipos de dados que s√£o retornados pela API.

## docker-compose.yml

Esse arquivo `docker-compose.yml` define uma aplica√ß√£o composta por tr√™s servi√ßos: `postgres`, `backend` e `frontend`, e cria uma rede chamada `mynetwork`. Vou explicar cada parte em detalhes:

### Services:

#### Postgres:

* `image: postgres:latest`: Esse servi√ßo utiliza a imagem mais recente do PostgreSQL dispon√≠vel no Docker Hub.
* `volumes`: Mapeia o diret√≥rio `/var/lib/postgresql/data` dentro do cont√™iner do PostgreSQL para um volume chamado `postgres_data` no sistema hospedeiro. Isso permite que os dados do banco de dados persistam mesmo quando o cont√™iner √© desligado.
* `environment`: Define vari√°veis de ambiente para configurar o banco de dados PostgreSQL, como nome do banco de dados (`POSTGRES_DB`), nome de usu√°rio (`POSTGRES_USER`) e senha (`POSTGRES_PASSWORD`).
* `networks`: Define que este servi√ßo est√° na rede chamada `mynetwork`.

#### Backend:

* `build`: Especifica que o Docker deve construir uma imagem para esse servi√ßo, usando um Dockerfile localizado no diret√≥rio `./backend`.
* `volumes`: Mapeia o diret√≥rio `./backend` (no sistema hospedeiro) para o diret√≥rio `/app` dentro do cont√™iner. Isso permite que as altera√ß√µes no c√≥digo fonte do backend sejam refletidas no cont√™iner em tempo real.
* `environment`: Define a vari√°vel de ambiente `DATABASE_URL`, que especifica a URL de conex√£o com o banco de dados PostgreSQL.
* `ports`: Mapeia a porta `8000` do sistema hospedeiro para a porta `8000` do cont√™iner, permitindo que o servi√ßo seja acessado atrav√©s da porta `8000`.
* `depends_on`: Indica que este servi√ßo depende do servi√ßo `postgres`, garantindo que o banco de dados esteja pronto antes que o backend seja iniciado.
* `networks`: Tamb√©m define que este servi√ßo est√° na rede `mynetwork`.

#### Frontend:

* `build`: Similar ao backend, especifica que o Docker deve construir uma imagem para este servi√ßo, usando um Dockerfile localizado no diret√≥rio `./frontend`.
* `volumes`: Mapeia o diret√≥rio `./frontend` (no sistema hospedeiro) para o diret√≥rio `/app` dentro do cont√™iner, permitindo altera√ß√µes em tempo real.
* `ports`: Mapeia a porta `8501` do sistema hospedeiro para a porta `8501` do cont√™iner, permitindo acesso ao frontend atrav√©s da porta `8501`.
* `networks`: Define que este servi√ßo tamb√©m est√° na rede `mynetwork`.

### Networks:

* `mynetwork`: Define uma rede personalizada para os servi√ßos se comunicarem entre si.

### Volumes:

* `postgres_data`: Define um volume para armazenar os dados do banco de dados PostgreSQL.

### Comando `docker-compose up`:

Quando voc√™ executa `docker-compose up`, o Docker Compose ler√° o arquivo `docker-compose.yml`, criar√° os servi√ßos conforme as defini√ß√µes especificadas e os iniciar√°. Isso significa que os cont√™ineres para o banco de dados PostgreSQL, o backend e o frontend ser√£o criados e conectados √† rede `mynetwork`. O banco de dados ser√° configurado com os detalhes fornecidos (nome do banco de dados, usu√°rio e senha), e as imagens para os servi√ßos de backend e frontend ser√£o constru√≠das a partir dos Dockerfiles fornecidos. Uma vez iniciados, voc√™ poder√° acessar o backend atrav√©s de `http://localhost:8000` e o frontend atrav√©s de `http://localhost:8501`. Os dados do banco de dados ser√£o persistidos no volume `postgres_data`.

## Nossa estrutura de pastas e arquivos

```bash
‚îú‚îÄ‚îÄ backend
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile # arquivo de configura√ß√£o do Docker
‚îÇ   ‚îú‚îÄ‚îÄ crud.py # arquivo com as fun√ß√µes de CRUD utilizando o SQL Alchemy ORM
‚îÇ   ‚îú‚îÄ‚îÄ database.py # arquivo com a configura√ß√£o do banco de dados utilizando o SQL Alchemy 
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ models.py
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ router.py
‚îÇ   ‚îî‚îÄ‚îÄ schemas.py
```

## Arquivo `database.py`

O arquivo `database.py` √© respons√°vel por fazer a configura√ß√£o do banco de dados utilizando o SQLAlchemy. Ele √© respons√°vel por criar a conex√£o com o banco de dados, e tamb√©m por criar a sess√£o do banco de dados.

Caso queira mudar de banco de dados, voc√™ s√≥ precisa mudar a URL de conex√£o, que est√° na vari√°vel SQLALCHEMY_DATABASE_URL. o SQLAlchemy √© compat√≠vel com v√°rios bancos de dados, como MySQL, PostgreSQL, SQLite, Oracle, Microsoft SQL Server, Firebird, Sybase e at√© mesmo o Microsoft Access.

Os principais pontos desse arquivo √© a engine, que √© a conex√£o com o banco de dados, e o SessionLocal, que √© a sess√£o do banco de dados. O SessionLocal √© quem executada as queries no banco de dados.

Lembrar sempre de:

1) Declarar a URL do banco
2) Criar a engine usando o 'create_engine'
3) Criar a sess√£o do banco
4) Criar a Base do ORM (nosso Model vai herdar ele)
5) Criar um gerador de sess√£o para ser reutilizado

## Arquivo `models.py`

O arquivo `models.py` √© respons√°vel por definir os modelos do SQLAlchemy, que s√£o as classes que definem as tabelas do banco de dados. Esses modelos s√£o utilizados para fazer a comunica√ß√£o com o banco de dados.

√â aqui que definimos o nome da tabela, os campos e os tipos de dados. Conseguimos incluir campos gerados aleatoriamente, como o id e o created_at. Para o id, ao incluir o campo Integer, com o par√¢metro primary_key=True, o SQLAlchemy j√° entende que esse campo √© o id da tabela. Para o created_at, ao incluir o campo DateTime, com o par√¢metro default=datetime, o SQLAlchemy j√° entende que esse campo √© a data de cria√ß√£o da tabela.

Lembrar:

1) O models √© agn√≥stico ao banco, ele n√£o sabe qual √© o banco que √© criado! Ele vai importar o base do database!

2) Declarar sua Tabela

## Arquivo `schemas.py`

O arquivo `schemas.py` √© respons√°vel por definir os schemas do Pydantic, que s√£o as classes que definem os tipos de dados que ser√£o utilizados na API. Esses schemas s√£o utilizados para fazer a valida√ß√£o dos dados que s√£o recebidos na API, e tamb√©m para definir os tipos de dados que s√£o retornados pela API.

O pydantic √© a principal biblioteca para valida√ß√£o de dados em Python. Ela √© utilizada pelo FastAPI para fazer a valida√ß√£o dos dados recebidos na API, e tamb√©m para definir os tipos de dados que s√£o retornados pela API.

Al√©m disso, ela possui uma integra√ß√£o muito boa com o SQLAlchemy, que √© a biblioteca que utilizamos para fazer a comunica√ß√£o com o banco de dados.

Outra vantagem s√£o os seus tipos pr√©-definidos, que facilitam muito a nossa vida. Por exemplo, se voc√™ quer definir um campo que aceita apenas n√∫meros positivos, voc√™ pode utilizar o PositiveInt. Se voc√™ quer definir um campo que aceita apenas determinadas categorias, voc√™ pode utilizar o construtor constrains.

Detalhe que criamos schemas diferentes para os retornos da nossa API. Isso √© uma boa pr√°tica, pois permite que voc√™ tenha mais flexibilidade para alterar os schemas no futuro.

Temos o schema `ProductBase`, que √© o schema base para o cadastro de produtos. Esse schema √© utilizado para fazer a valida√ß√£o dos dados que s√£o recebidos na API, e tamb√©m para definir os tipos de dados que s√£o retornados pela API.

Temos o schema `ProductCreate`, que √© o schema que √© retornado pela API. Ele √© uma classe que herda do schema `ProductBase`, e possui um campo a mais, que √© o id. Esse campo √© utilizado para identificar o produto no banco de dados.

Temos o schema `ProductResponse`, que √© o schema que √© retornado pela API. Ele √© uma classe que herda do schema `ProductBase`, e possui dois campos a mais, que √© o id e o created_at. Esses campos s√£o gerados pelo nosso banco de dados.

Temos o schema `ProductUpdate`, que √© o schema que √© recebido pela API para update. Ele possui os campos opcionais, pois n√£o √© necess√°rio enviar todos os campos para fazer o update.

## Arquivo `crud.py`

O arquivo `crud.py` √© respons√°vel por definir as fun√ß√µes de CRUD utilizando o SQLAlchemy ORM. Essas fun√ß√µes s√£o utilizadas para fazer a comunica√ß√£o com o banco de dados. √â nele que definimos as fun√ß√µes de listagem, cria√ß√£o, atualiza√ß√£o e remo√ß√£o de produtos. √â onde os dados s√£o persistidos no banco de dados.

## Arquivo `router.py`

O arquivo `router.py` √© respons√°vel por definir as rotas da API utilizando o FastAPI. √â aqui que definimos as rotas, e tamb√©m as fun√ß√µes que ser√£o executadas em cada rota. Todas as fun√ß√µes definidas aqui recebem um par√¢metro, que √© o par√¢metro request, que √© o objeto que cont√©m os dados da requisi√ß√£o.

Os principais parametros s√£o o path, que √© o caminho da rota, o methods, que s√£o os m√©todos HTTP que a rota aceita, e o response_model, que √© o schema que √© retornado pela rota.

```python
@router.post("/products/", response_model=ProductResponse)
```
Importante destacar que o FastAPI utiliza o conceito de type hints, que s√£o as anota√ß√µes de tipos. Isso permite que o FastAPI fa√ßa a valida√ß√£o dos dados que s√£o recebidos na API, e tamb√©m para definir os tipos de dados que s√£o retornados pela API. Por exemplo, ao definir o par√¢metro product do tipo ProductResponse, o FastAPI j√° entende que os dados recebidos nesse par√¢metro devem ser do tipo ProductResponse.

Conseguimos tamb√©m retornar par√¢metros pelo nosso path, no caso do delete, por exemplo, precisamos passar o id do produto que queremos deletar. Para isso, utilizamos o path /products/{product_id}, e definimos o par√¢metro product_id na fun√ß√£o delete_product.

```python
@router.get("/products/{product_id}", response_model=ProductResponse)
def read_product_route(product_id: int, db: Session = Depends(get_db)):
    db_product = get_product(db, product_id=product_id)
    if db_product is None:
        raise HTTPException(status_code=404, detail="Product not found")
    return db_product
```

## Arquivo `main.py`

O arquivo `main.py` √© respons√°vel por definir a aplica√ß√£o do FastAPI, e tamb√©m por definir o servidor web Uvicorn. √â aqui que definimos o servidor web, e tamb√©m as configura√ß√µes do servidor web, como o host e a porta.


## Nosso Frontend

Nosso frontend vai ser uma aplica√ß√£o que vai consumir a nossa API, e vai ser respons√°vel por fazer o cadastro, altera√ß√£o e remo√ß√£o de produtos. Vamos detalhar cada uma das pastas e arquivos do nosso frontend.

### Streamlit

O Streamlit √© uma biblioteca para construir aplica√ß√µes web com Python. Ele √© muito utilizado para construir dashboards, e tamb√©m para construir aplica√ß√µes que consomem APIs.

### Requests

O Requests √© uma biblioteca para fazer requisi√ß√µes HTTP com Python. Ele √© muito utilizado para consumir APIs, e tamb√©m para fazer web scraping.

### Pandas

O Pandas √© uma biblioteca para manipula√ß√£o de dados com Python. Ele √© muito utilizado para fazer an√°lise de dados, e tamb√©m para construir dashboards.



## Deploy <> Em constru√ß√£o





### AWS ECS

Al√©m disso, nesse projeto vamos apresentar como colocar em produ√ß√£o um projeto utilizando containers Docker, utilizando o AWS ECS (Amazon Elastic Container Service).

Se voc√™ quer ter toda a facilidade do Docker, garantir que o seu ambiente de desenvolvimento e de produ√ß√£o s√£o id√™nticos, e ainda ter a possibilidade de escalar a sua aplica√ß√£o, esse projeto √© para voc√™.

A AWS ECS √© um servi√ßo de orquestra√ß√£o de containers, que permite que voc√™ execute containers Docker de forma escal√°vel e altamente dispon√≠vel. Com ele, voc√™ n√£o precisa se preocupar com a infraestrutura, pois a AWS cuida de tudo para voc√™.

### AMAZON ECS

√â um servi√ßo de orquestra√ß√£o de containers, que permite que voc√™ execute containers Docker de forma escal√°vel e altamente dispon√≠vel. A vantagem principal √© que voc√™ n√£o precisa se preocupar com a orquestra√ß√£o dos containers (Kubernetes) mas tenha todas as vantagens de utilizar containers Docker.

### AMAZON ECS FARGATE

O ECS Fargate √© um servi√ßo que permite que voc√™ execute containers Docker sem precisar gerenciar servidores. Ou seja, todo o gerenciamento de servidores, balanceamento de carga, auto scaling, etc, √© feito pela AWS. √â um servi√ßo ainda mais gerenciado que o ECS, pois voc√™ n√£o precisa se preocupar com a infraestrutura.

### Conceitos

[Imagem arquitetura](assets/arquitetura.png)

#### Cluster

Um cluster √© um grupo de inst√¢ncias EC2 (m√°quinas) que executam as suas tarefas. Ou seja, as m√°quinas onde os meus containers v√£o ser executados.

#### Task Definition

Uma task definition √© um arquivo de configura√ß√£o (com a formata√ß√£o JSON) que define como a sua aplica√ß√£o vai ser executada. Nesse arquivo voc√™ define qual imagem Docker vai ser utilizada, qual o poder computacional necess√°rio, qual o volume que vai ser utilizado, etc.

#### Task

Uma task √© uma inst√¢ncia de uma task definition. Ou seja, √© uma execu√ß√£o da sua aplica√ß√£o. Por exemplo, se voc√™ tem uma task definition que define que a sua aplica√ß√£o vai ser executada com 2 inst√¢ncias, voc√™ ter√° 2 tasks executando a sua aplica√ß√£o. Aplicado ao Airflow que vimos no Workshop 02, podemos subir mais de uma inst√¢ncia do Airflow, para garantir que a nossa aplica√ß√£o vai estar sempre dispon√≠vel. Al√©m disso, podemos configurar para subir mais inst√¢ncias quando a CPU estiver alta, por exemplo.

#### Service

Um service √© um grupo de tasks que s√£o executadas juntas. Por exemplo, se voc√™ tem uma task definition que define que a sua aplica√ß√£o vai ser executada com 2 inst√¢ncias, voc√™ ter√° 2 tasks executando a sua aplica√ß√£o. Essas 2 tasks formam um service. Se alguma tarefa falhar, o service vai garantir que ela vai ser executada novamente. O service tamb√©m pode ser utilizado para balancear a carga entre as tasks.

